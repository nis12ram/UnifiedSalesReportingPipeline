{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "142b30df-7bfb-48a0-848f-3f53f8d547cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql import types as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c060275f-296d-463e-ad0d-02c86479df78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "possible_source_timestamp_formats = [\n",
    "    \"yyyy-MM-dd HH:mm:ss\",\n",
    "    \"yyyy-MM-dd'T'HH:mm:ss\",\n",
    "    \"yyyy-MM-dd'T'HH:mm:ss.SSS\",\n",
    "    \"MM-dd-yyyy HH:mm:ss\",\n",
    "    \"MM/dd/yyyy HH:mm:ss\",\n",
    "    \"dd-MM-yyyy HH:mm:ss\",\n",
    "    \"M/d/yyyy h:mm:ss a\",\n",
    "    \"MMMM d, yyyy h:mm:ss a\",\n",
    "]\n",
    "\n",
    "def parse_timestamp_expr(col_name: str) -> pyspark.sql.Column:\n",
    "    \"\"\"\n",
    "    Attempts to parse a timestamp column using multiple possible timestamp formats.\n",
    "    Trims whitespace and tries each format in order, returning the first successfully parsed TimestampType.\n",
    "    If none match, returns null.\n",
    "    \"\"\"\n",
    "    ts_exprs = [\n",
    "        sf.try_to_timestamp(\n",
    "            sf.trim(sf.col(col_name)),\n",
    "            sf.lit(possible_source_timestamp_format)\n",
    "        )\n",
    "        for possible_source_timestamp_format in possible_source_timestamp_formats\n",
    "    ]\n",
    "    return sf.coalesce(*ts_exprs)\n",
    "\n",
    "## Test\n",
    "def test_parse_timestamp_expr():\n",
    "    test_data = [\n",
    "        (\"2024-06-01 14:30:00\",),\n",
    "        (\"2024-06-01T14:30:00\",),\n",
    "        (\"06-01-2024 02:30:00\",),\n",
    "        (\"06/01/2024 02:30:00\",),\n",
    "        (\"June 1, 2024 2:30:00 PM\",),\n",
    "        (\"InvalidTS\",),\n",
    "        (None,),\n",
    "        (\"   \",)\n",
    "    ]\n",
    "    test_df = spark.createDataFrame(test_data, [\"SalesTimestamp\"])\n",
    "    test_df = test_df.withColumn(\n",
    "        \"ParsedTimestamp\",\n",
    "        parse_timestamp_expr(\"SalesTimestamp\")\n",
    "    )\n",
    "    display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff57c642-e273-4366-830e-8178f4c47009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "possible_source_date_formats = [\n",
    "    \"yyyy-MM-dd\",\n",
    "    \"MM-dd-yyyy\",\n",
    "    \"MM/dd/yyyy\",\n",
    "    \"dd-MM-yyyy\",\n",
    "    \"M/d/yyyy\",\n",
    "    \"MMMM d, yyyy\"\n",
    "]\n",
    "def parse_date_expr(col_name: str) -> pyspark.sql.Column:\n",
    "    \"\"\"\n",
    "    Attempts to parse a date column using multiple possible source date formats.\n",
    "    Trims whitespace from the column, tries each format in order, and returns the first successfully parsed date in yyyy-MM-dd format.\n",
    "    If none of the formats match, returns null for that row.\n",
    "\n",
    "    Args:\n",
    "        col_name (str): The name of the column containing date strings.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.Column: A column expression with values cast to DateType, or null if parsing fails.\n",
    "    \"\"\"\n",
    "    date_expr = [\n",
    "        sf.try_to_date(\n",
    "            sf.trim(\n",
    "                sf.col(col_name)\n",
    "            ),\n",
    "            possible_source_date_format\n",
    "        ) \n",
    "        for possible_source_date_format in possible_source_date_formats\n",
    "    ]\n",
    "    return sf.coalesce(*date_expr)\n",
    "\n",
    "## Test\n",
    "def test_parse_date_expr():\n",
    "    test_data = [\n",
    "        (\"2024-06-01\",),\n",
    "        (\"12-21-2025\",),\n",
    "        (\"06-01-2024\",),\n",
    "        (\"01-06-2024\",),\n",
    "        (\"06/01/2024\",),\n",
    "        (\"invalid\",),\n",
    "        (\" 2024-06-01 \",),\n",
    "        (None,),\n",
    "        (\"\",)\n",
    "    ]\n",
    "    test_df = spark.createDataFrame(test_data, [\"SalesDate\"])\n",
    "    test_df = test_df.withColumn(\"ParsedDate\", parse_date_expr(\"SalesDate\"))\n",
    "    display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ef3462-b62c-4343-be65-4022b19f07a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_int_expr(col_name: str) -> pyspark.sql.Column:\n",
    "    \"\"\"\n",
    "    Cleans and safely casts a string column to IntegerType.\n",
    "\n",
    "    Removes commas and whitespace from the specified column, then attempts to cast the cleaned value to IntegerType.\n",
    "    If the value is null, returns the original value.\n",
    "\n",
    "    Args:\n",
    "        col_name (str): The name of the column containing integer-like strings.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.Column: A column expression with values cast to IntegerType, or the original value if null.\n",
    "    \"\"\"\n",
    "    intermediate_col = sf.regexp_replace(sf.col(col_name), r\"[,\\s]\", \"\")\n",
    "    intermediate_col = sf.regexp_replace(intermediate_col, r\"\\.\\d*$\", \"\")\n",
    "    \n",
    "    cleansed_col = sf.when(\n",
    "        sf.col(col_name).isNotNull(),\n",
    "        intermediate_col\n",
    "    ).otherwise(sf.col(col_name))\n",
    "\n",
    "    return cleansed_col.try_cast(st.IntegerType())\n",
    "\n",
    "## Test\n",
    "def test_cast_int_expr():\n",
    "    test_df = spark.createDataFrame(\n",
    "        [(\"1,234\",), (\" 56 78 \",), (\"100.0\",), (\"\",), (None,)],\n",
    "        [\"raw\"]\n",
    "    )\n",
    "\n",
    "    test_df = test_df.withColumn(\"clean_int\", cast_int_expr(\"raw\"))\n",
    "    display(test_df)\n",
    "# test_cast_int_expr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ca2bd8-76f5-48e0-8ad7-0ad6e08c5fd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_double_expr(col_name: str) -> pyspark.sql.Column:\n",
    "    \"\"\"\n",
    "    Cleans and safely casts a string column to DoubleType.\n",
    "\n",
    "    Removes commas and whitespace from the specified column, then attempts to cast the cleaned value to DoubleType.\n",
    "    If the value is null, returns the original value.\n",
    "\n",
    "    Args:\n",
    "        col_name (str): The name of the column containing double-like strings.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.Column: A column expression with values cast to DoubleType, or the original value if null.\n",
    "    \"\"\"\n",
    "    cleansed_col = sf.when(\n",
    "        sf.col(col_name).isNotNull(),\n",
    "        sf.regexp_replace(\n",
    "            sf.col(col_name),\n",
    "            r\"[,\\s]\",\n",
    "            \"\"\n",
    "        )\n",
    "    ).otherwise(sf.col(col_name))\n",
    "\n",
    "    return cleansed_col.try_cast(st.DoubleType())\n",
    "\n",
    "## Test\n",
    "def test_cast_double_expr():\n",
    "    test_df = spark.createDataFrame(\n",
    "        [(\"1,234.50\",), (\" 56 78 . 24 \",), (\"100.20\",) ,(\" \",), (None,)],\n",
    "        [\"raw\"]\n",
    "    )\n",
    "\n",
    "    test_df = test_df.withColumn(\"clean_double\", cast_double_expr(\"raw\"))\n",
    "    display(test_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
